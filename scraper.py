import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
from typing import List, Dict
import random
import streamlit as st

class PokerfansScraper:
    def __init__(self, target_date: str = None):
        """
        Args:
            target_date (str, optional): 'YYYY/MM/DD'å½¢å¼ã®æ—¥ä»˜æ–‡å­—åˆ—ã€‚
                                       Noneã®å ´åˆã¯ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã€‚
        """
        # target_dateãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨
        date_str = target_date or datetime.now().strftime('%Y/%m/%d')
        
        # ãƒ™ãƒ¼ã‚¹URLã¨ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢
        self.base_url = "https://pokerfans.jp/"
        self.params = {
            "startDate": date_str,
            "weekly": "false",
            "prize": "",
            "location": "æ±äº¬éƒ½",
            "clubId": "",
            "withEndTime": "false",
            "applyEndTime": "",
            "size": "50"
        }
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥: è©³ç´°ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ä¿å­˜
        self._detail_cache = {}

    def _random_delay(self, min_seconds=3, max_seconds=7):
        """ãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚é–“å¾…æ©Ÿã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ã‚’å›é¿"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)

    def _extract_number(self, text: str) -> int:
        """æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã®ã¿ã‚’æŠ½å‡º"""
        if not text:
            return 0
        numbers = re.findall(r'\d+', text.replace(',', ''))
        return int(numbers[0]) if numbers else 0

    def _extract_guarantee(self, text: str) -> int:
        """
        æ–‡å­—åˆ—ã‹ã‚‰ä¿è¨¼é¡ã‚’æŠ½å‡ºã—ã¦å††å˜ä½ã§è¿”ã™ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰
        Args:
            text: ä¿è¨¼é¡ã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹æ–‡å­—åˆ—
        Returns:
            int: ä¿è¨¼é¡ï¼ˆå††å˜ä½ï¼‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0
        """
        if not text:
            return 0
        
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
        
        # ä¿è¨¼é¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            # "ä¸‡"ã‚’å«ã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹ï¼š84ä¸‡ç›¸å½“ã€40ä¸‡ã‚³ã‚¤ãƒ³ã€7ä¸‡å††ä¿è¨¼ï¼‰
            (r'(?:ç·é¡)?(\d+)ä¸‡(?:å††|coin|ã‚³ã‚¤ãƒ³)?(?:ç›¸å½“|ä¿è¨¼)?', lambda x: int(x) * 10000),
            
            # æ•°å€¤+é€šè²¨å˜ä½ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹ï¼š50,000coinã€100,000ã‚³ã‚¤ãƒ³ï¼‰
            (r'(?:Web|ã‚¦ã‚§ãƒ–)?(?:æœ€ä½ä¿è¨¼)?[^\d]*(\d+[,\d]*)(?:å††|coin|ã‚³ã‚¤ãƒ³)(?:ä¿è¨¼)?',
             lambda x: int(x.replace(',', ''))),
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern, converter in patterns:
            match = re.search(pattern, text, re.IGNORECASE)  # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„
            if match:
                try:
                    return converter(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        return 0

    def _extract_guarantee_from_detail(self, text: str) -> int:
        """
        è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¿è¨¼é¡ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå³å¯†ãªæ¡ä»¶ã§ï¼‰
        Args:
            text: è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
        Returns:
            int: ä¿è¨¼é¡ï¼ˆå††å˜ä½ï¼‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0
        """
        if not text:
            return 0
        
        # å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
        text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ç”¨ã®å³å¯†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            # "ä¿è¨¼"ã¨ã„ã†å˜èªãŒå¿…ãšå«ã¾ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
            
            # â—â—ä¸‡å††ä¿è¨¼
            (r'(\d+)ä¸‡å††ä¿è¨¼', lambda x: int(x) * 10000),
            
            # â—â—ä¸‡ä¿è¨¼
            (r'(\d+)ä¸‡ä¿è¨¼', lambda x: int(x) * 10000),
            
            # â—â—,â—â—â—å††ä¿è¨¼
            (r'(\d+[,\d]*)å††ä¿è¨¼', lambda x: int(x.replace(',', ''))),
            
            # â—â—ä¸‡ã‚³ã‚¤ãƒ³ä¿è¨¼
            (r'(\d+)ä¸‡(?:coin|ã‚³ã‚¤ãƒ³)ä¿è¨¼', lambda x: int(x) * 10000),
            
            # â—â—,â—â—â—ã‚³ã‚¤ãƒ³ä¿è¨¼
            (r'(\d+[,\d]*)(?:coin|ã‚³ã‚¤ãƒ³)ä¿è¨¼', lambda x: int(x.replace(',', ''))),
            
            # ç·é¡â—â—ä¸‡ä¿è¨¼
            (r'ç·é¡(\d+)ä¸‡ä¿è¨¼', lambda x: int(x) * 10000),
            
            # æœ€ä½ä¿è¨¼â—â—ä¸‡
            (r'æœ€ä½ä¿è¨¼[^\d]*(\d+)ä¸‡', lambda x: int(x) * 10000),
            
            # æœ€ä½ä¿è¨¼â—â—,â—â—â—ã‚³ã‚¤ãƒ³
            (r'æœ€ä½ä¿è¨¼[^\d]*(\d+[,\d]*)(?:coin|ã‚³ã‚¤ãƒ³)', lambda x: int(x.replace(',', ''))),
        ]
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ï¼ˆæ”¹è¡Œã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€re.MULTILINE | re.IGNORECASE ã‚’ä½¿ç”¨ï¼‰
        for pattern, converter in patterns:
            match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
            if match:
                try:
                    return converter(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        return 0

    def get_tournament_list(self, page: int = 0, max_details_per_page: int = 5) -> tuple[List[Dict], Dict]:
        """
        ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—æ•°ã‚’åˆ¶é™ï¼‰
        Args:
            page: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ0-basedï¼‰
            max_details_per_page: 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šå–å¾—ã™ã‚‹è©³ç´°ãƒšãƒ¼ã‚¸ã®æœ€å¤§æ•°ï¼ˆ0=ã™ã¹ã¦å–å¾—ã—ãªã„ï¼‰
        """
        tournaments = []
        
        # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®š
        params = self.params.copy()
        params["page"] = str(page)
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚ˆã‚Šé•·ã„å¾…æ©Ÿæ™‚é–“ï¼‰
        try:
            response = requests.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=30
            )
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        except requests.RequestException as e:
            print(f"ä¸€è¦§ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {e}")
            return [], {"current_page": page, "total_pages": 1}
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
        pagination_info = self._get_pagination_info(soup)
        
        # ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆæƒ…å ±ã®å–å¾—
        details_count = 0
        
        for event in soup.select('.profile-event'):
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨è©³ç´°ãƒšãƒ¼ã‚¸URL
                title_link = event.select_one('h5 > a.color-green.tooltips')
                if not title_link:
                    continue
                    
                title = title_link.text.strip()
                detail_url = self.base_url.rstrip('/') + title_link['href']
                
                # ãƒªãƒ³ã‚°ã‚²ãƒ¼ãƒ ã¯é™¤å¤–
                if any(keyword in title for keyword in ['ãƒªãƒ³ã‚°', 'ã‚³ã‚¤ãƒ³ãƒªãƒ³ã‚°', 'RING', 'Ring', 'ring']):
                    continue
                
                # æ–½è¨­å
                venue_spans = event.select('div.oneline span')
                venue = venue_spans[1].text.strip() if len(venue_spans) > 1 else ''            
                # é–‹å§‹ãƒ»ç· åˆ‡æ™‚åˆ»ã®æŠ½å‡º
                time_text = event.select_one('strong.text-danger').text.strip()
                print(f"ğŸ•’ time_text raw: '{time_text}'")  # ãƒ‡ãƒãƒƒã‚°ç”¨

                # é–‹å§‹æ™‚é–“ã®æŠ½å‡ºï¼ˆæœ€åˆã«å‡ºã¦ãã‚‹æ™‚é–“ï¼‰
                start_time_match = re.search(r'(\d{2}:\d{2})', time_text)

                # ç· åˆ‡æ™‚é–“ã®æŠ½å‡º - ã€†ã¾ãŸã¯Endã®å¾Œã‚ã®æ™‚é–“
                end_time_match = re.search(r'(?:ã€†|End)(\d{2}:\d{2})', time_text)

                start_time = start_time_match.group(1) if start_time_match else None
                end_time = end_time_match.group(1) if end_time_match else None
                
                # ã‚¨ãƒ³ãƒˆãƒªãƒ¼è²»
                entry_fee_text = ''
                for span in event.select('div.col-xs-6 span'):
                    text = span.text.strip()
                    if re.search(r'\(?E\)?|ã‚¨ãƒ³ãƒˆãƒªãƒ¼|å‚åŠ è²»|Â¥|å††|ï¿¥', text):
                        entry_fee_text = text
                        break
                entry_fee = self._extract_number(entry_fee_text)
                
                # ã‚¨ãƒ³ãƒˆãƒªãƒ¼äººæ•°ï¼å®šå“¡ã®å‡¦ç†ã‚’ä¿®æ­£
                try:
                    entry_count_text = event.select_one('i.icon-users + span').text.strip()
                    # "0 /" ã‚„ "0 / " ã®ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã«å¯¾å¿œ
                    parts = [p.strip() for p in entry_count_text.split('/')]
                    current_entries = int(parts[0]) if parts[0].strip() else 0
                    max_entries = int(parts[1]) if len(parts) > 1 and parts[1].strip() else 0
                except (ValueError, AttributeError, IndexError):
                    current_entries = 0
                    max_entries = 0
                    print(f"Warning: Could not parse entry count from text: {entry_count_text}")
                # venue æŠ½å‡º
                venue_spans = event.select('div.oneline span')
                venue = venue_spans[1].text.strip() if len(venue_spans) > 1 else ''

                # âœ… ãƒ•ã‚£ãƒ«ã‚¿ï¼šæ±äº¬éƒ½ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
                if "æ±äº¬éƒ½" not in venue:
                    continue
                
                tournament_info = {
                    'title': title,
                    'venue': venue,
                    'start_time': start_time,
                    'end_time': end_time,
                    'entry_fee': entry_fee,
                    'current_entries': current_entries,
                    'max_entries': max_entries,
                    'detail_url': detail_url
                }
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¿è¨¼é¡ã‚’å–å¾—
                guarantee = self._extract_guarantee(title)
                tournament_info['guarantee'] = guarantee
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å–å¾—ã§ããšã€è©³ç´°å–å¾—æ•°ãŒä¸Šé™æœªæº€ã®å ´åˆã®ã¿è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
                # if guarantee == 0 and (max_details_per_page == 0 or details_count < max_details_per_page):
                #     detail_info = self.get_tournament_detail(detail_url)
                #     tournament_info['guarantee'] = detail_info['guarantee']
                #     details_count += 1
                #     self._random_delay()  # è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¾Œã«å¾…æ©Ÿ
                
                tournaments.append(tournament_info)
                
            except Exception as e:
                print(f"ãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆæƒ…å ±ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ã‚ˆã‚Šé•·ãå¾…æ©Ÿ
        self._random_delay(5, 10)
        
        return tournaments, pagination_info

    def _get_pagination_info(self, soup) -> Dict:
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—"""
        pagination = soup.select_one('ul.pagination')
        if not pagination:
            return {'current_page': 0, 'total_pages': 1}
        
        current_page = int(pagination.select_one('li.active a').text) - 1
        page_numbers = [int(a.text) for a in pagination.select('li a') if a.text.isdigit()]
        total_pages = max(page_numbers) if page_numbers else 1
        
        return {'current_page': current_page, 'total_pages': total_pages}

    def get_tournament_detail(self, url: str) -> Dict:
        """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°ãã‚Œã‚’è¿”ã™
        if url in self._detail_cache:
            return self._detail_cache[url]
            
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            detail_text = soup.select_one('pre.pre-white').text if soup.select_one('pre.pre-white') else ''
            
            # ä¿è¨¼è³é‡‘ã®æŠ½å‡º
            guarantee = self._extract_guarantee_from_detail(detail_text)
            
            result = {'guarantee': guarantee}
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._detail_cache[url] = result
            return result
            
        except Exception as e:
            print(f"è©³ç´°ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {url} - {e}")
            return {'guarantee': 0}


